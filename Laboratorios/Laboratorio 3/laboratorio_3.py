# -*- coding: utf-8 -*-
"""Laboratorio 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bc_blY19OQeQ7LonvSRvJUoXLXbiJuXq

# Laboratorio 3. Clasificación multiclase.
Rúa Echalar Juan Manuel
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# utilizado para la manipulación de directorios y rutas
import os

# Cálculo científico y vectorial para python
import numpy as np

# Libreria para graficos
from matplotlib import pyplot

# Modulo de optimizacion en scipy
from scipy import optimize

# modulo para cargar archivos en formato MATLAB
# from scipy.io import loadmat

# le dice a matplotlib que incruste gráficos en el cuaderno
# %matplotlib inline

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Cargar el dataset
data = pd.read_csv('/content/drive/MyDrive/IA/SIS420/Clase_pasajero_test.csv')
# Eliminar la columna 'id' si no es relevante para el análisis
data.drop('#', axis=1, inplace=True)

# Codificar la columna 'Gender' usando LabelEncoder
label_encoder = LabelEncoder()
data['Gender'] = label_encoder.fit_transform(data['Gender'])

# Codificar la columna 'Customer Type' usando LabelEncoder
data['Customer Type'] = label_encoder.fit_transform(data['Customer Type'])

# Codificar la columna 'Type of Travel' usando LabelEncoder
data['Type of Travel'] = label_encoder.fit_transform(data['Type of Travel'])

# Convertir 'satisfaction' a 1 para 'satisfied' y 0 para 'neutral or dissatisfaction'
data['satisfaction'] = (data['satisfaction'] == 'satisfied').astype(int)

# Reorganizar las columnas para poner 'Class' al final
class_col = data.pop('Class')
data['Class'] = class_col

# Guardar el dataset preprocesado en un nuevo archivo CSV
data.to_csv('dataset_preprocesado_numerico.csv', index=False)

# Cargar el dataset usando Pandas
data = pd.read_csv('dataset_preprocesado_numerico.csv')

# Eliminar la primera fila que contiene los encabezados de las columnas
data = data.iloc[1:]

# Convertir la columna 'Class' en valores numéricos
class_mapping = {'Business': 1, 'Eco': 2, 'Eco Plus': 3}
data['Class'] = data['Class'].map(class_mapping)

# Eliminar las filas que contienen valores faltantes
data.dropna(inplace=True)

# Convertir el DataFrame a una matriz NumPy si es necesario
data_np = data.to_numpy()

# Ahora puedes acceder a 'data_np' para obtener tus datos como una matriz NumPy

# La entrada es de 23 elemento contando con x0
input_layer_size  = 23

# 3 etiquetas (1, 2 o 3)
num_labels = 3

X = data_np[:20714, :-1]
y = data_np[:20714, -1]
X_test = data_np[20714:, :-1]
Y_test = data_np[20714:, -1]

m = y.size

print(X.shape)

print(X[0,:])
print(y)

def  featureNormalize(X):
    X_norm = X.copy()
    mu = np.zeros(X.shape[1])
    sigma = np.zeros(X.shape[1])

    mu = np.mean(X, axis = 0)
    sigma = np.std(X, axis = 0)
    X_norm = (X - mu) / sigma

    return X_norm, mu, sigma

# llama featureNormalize con los datos cargados
X_norm, mu, sigma = featureNormalize(X)
X_test_norm, mu_test,sigma_test = featureNormalize(X_test)

# Configurar la matriz adecuadamente, y agregar una columna de unos que corresponde al termino de intercepción.
m, n = X.shape
X = X_norm

print(X.shape)

# Selecciona aleatoriamente 100 puntos de datos para mostrar
rand_indices = np.random.choice(m, 100, replace=False)
sel = X[rand_indices, :]

def sigmoid(z):
    """
    Calcula la sigmoide de z.
    """
    return 1.0 / (1.0 + np.exp(-z))

def lrCostFunction(theta, X, y, lambda_):
    """
    Calcula el costo de usar theta como parámetro para la regresión logística regularizada y
    el gradiente del costo w.r.t. a los parámetros.

    Parametros
    ----------
    theta : array_like
        Parametro theta de la regresion logistica. Vector de la forma(shape) (n, ). n es el numero de caracteristicas
        incluida la intercepcion

    X : array_like
        Dataset con la forma(shape) (m x n). m es el numero de ejemplos, y n es el numero de
        caracteristicas (incluida la intercepcion).

    y : array_like
        El conjunto de etiquetas. Un vector con la forma (shape) (m, ). m es el numero de ejemplos

    lambda_ : float
        Parametro de regularización.

    Devuelve
    -------
    J : float
        El valor calculado para la funcion de costo regularizada.

    grad : array_like
        Un vector de la forma (shape) (n, ) que es el gradiente de la
        función de costo con respecto a theta, en los valores actuales de theta..
    """

    # Inicializa algunos valores utiles
    m = y.size

    # convierte las etiquetas a valores enteros si son boleanos
    if y.dtype == bool:
        y = y.astype(int)

    J = 0
    grad = np.zeros(theta.shape)

    h = sigmoid(X.dot(theta.T))

    temp = theta
    temp[0] = 0

#     J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h)))
    J = (1 / m) * np.sum(-y.dot(np.log(h)) - (1 - y).dot(np.log(1 - h))) + (lambda_ / (2 * m)) * np.sum(np.square(temp))

    grad = (1 / m) * (h - y).dot(X)
#     theta = theta - (alpha / m) * (h - y).dot(X)
    grad = grad + (lambda_ / m) * temp

    return J, grad
#    return J, theta

def oneVsAll(X, y, num_labels, lambda_):
    """
    Trains num_labels logistic regression classifiers and returns
    each of these classifiers in a matrix all_theta, where the i-th
    row of all_theta corresponds to the classifier for label i.

    Parameters
    ----------
    X : array_like
        The input dataset of shape (m x n). m is the number of
        data points, and n is the number of features. Note that we
        do not assume that the intercept term (or bias) is in X, however
        we provide the code below to add the bias term to X.

    y : array_like
        The data labels. A vector of shape (m, ).

    num_labels : int
        Number of possible labels.

    lambda_ : float
        The logistic regularization parameter.

    Returns
    -------
    all_theta : array_like
        The trained parameters for logistic regression for each class.
        This is a matrix of shape (K x n+1) where K is number of classes
        (ie. `numlabels`) and n is number of features without the bias.
    """
    # algunas variables utiles
    m, n = X.shape

    all_theta = np.zeros((num_labels, n + 1))

    # Agrega unos a la matriz X
    X = np.concatenate([np.ones((m, 1)), X], axis=1)

    for c in np.arange(num_labels):
        initial_theta = np.zeros(n + 1)
        options = {'maxiter': 50}
        res = optimize.minimize(lrCostFunction,
                                initial_theta,
                                (X, (y == c), lambda_),
                                jac=True,
                                method='CG',
                                options=options)

        all_theta[c] = res.x

    return all_theta

lambda_ = 0.1
all_theta = oneVsAll(X, y, num_labels, lambda_)
print(all_theta.shape)

print(all_theta)

def predictOneVsAll(all_theta, X):
    """
    Devuelve un vector de predicciones para cada ejemplo en la matriz X.
    Tenga en cuenta que X contiene los ejemplos en filas.
    all_theta es una matriz donde la i-ésima fila es un vector theta de regresión logística entrenada para la i-ésima clase.
    Debe establecer p en un vector de valores de 0..K-1 (por ejemplo, p = [0, 2, 0, 1]
    predice clases 0, 2, 0, 1 para 4 ejemplos).

    Parametros
    ----------
    all_theta : array_like
        The trained parameters for logistic regression for each class.
        This is a matrix of shape (K x n+1) where K is number of classes
        and n is number of features without the bias.

    X : array_like
        Data points to predict their labels. This is a matrix of shape
        (m x n) where m is number of data points to predict, and n is number
        of features without the bias term. Note we add the bias term for X in
        this function.

    Devuelve
    -------
    p : array_like
        The predictions for each data point in X. This is a vector of shape (m, ).
    """

    m = X.shape[0];
    num_labels = all_theta.shape[0]

    p = np.zeros(m)

    # Add ones to the X data matrix
    X = np.concatenate([np.ones((m, 1)), X], axis=1)
    p = np.argmax(sigmoid(X.dot(all_theta.T)), axis = 1)

    return p

print(X_norm)

print(X.shape)
pred = predictOneVsAll(all_theta, X)
print(X.shape)
print(X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
XPrueba = X[10:150, :].copy()
print(XPrueba.shape)

XPrueba = np.concatenate([np.ones((140, 1)), XPrueba], axis=1)
print(XPrueba.shape)
p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)
print(p)

print(y[10:150])

print(X.shape)
pred = predictOneVsAll(all_theta, X)
print('Precision del conjuto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
XPrueba = X_test_norm[10:150, :].copy()
print(XPrueba.shape)

XPrueba = np.concatenate([np.ones((140, 1)), XPrueba], axis=1)
print(XPrueba.shape)
p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis = 1)
print(p)

# displayData(X[1002:1003, :])
print(Y_test[10:150])

print(X.shape)
pred = predictOneVsAll(all_theta, X)
print('Precision del conjunto de entrenamiento: {:.2f}%'.format(np.mean(pred == y) * 100))
XPrueba = X_test_norm[10:150, :].copy()
print(XPrueba.shape)
XPrueba = np.concatenate([np.ones((140, 1)), XPrueba], axis=1)
p = np.argmax(sigmoid(XPrueba.dot(all_theta.T)), axis=1)

# Imprimir la clase predicha y la clase real
correctas = 0
total = len(Y_test[10:110])
for predicha, real in zip(p, y[10:110]):
    if predicha == real:
        correctas += 1
    print("Clase predicha para el pasajero: {}, Clase real: {}".format(predicha, real))

# Calcular y mostrar la precisión
precision = correctas / total
print("Correctas: {}/{}".format(correctas, total))